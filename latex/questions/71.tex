\section{What is word embedding?}
Word embedding is a technique used in natural language processing to represent words as real-valued vectors in a high-dimensional space. The goal of word embedding is to capture the semantic and syntactic relationships between words in a way that can be used by machine learning models.

Word embedding is typically learned from a large corpus of text data using unsupervised learning techniques such as neural networks. The basic idea is to represent each word as a dense vector in a high-dimensional space, where each dimension of the vector represents a different feature of the word. Words that are semantically or syntactically similar are expected to have similar vector representations, which allows machine learning models to capture the underlying relationships between words.

One popular method for learning word embeddings is Word2Vec, which uses a neural network to predict the context of a word given its neighboring words. The resulting word embeddings are learned in such a way that words that appear in similar contexts have similar vector representations.

Word embeddings can be used in a wide range of natural language processing tasks, including language modeling, text classification, and machine translation. They have been shown to be effective in improving the performance of machine learning models by capturing the underlying relationships between words in a more meaningful way than traditional one-hot encoding.

